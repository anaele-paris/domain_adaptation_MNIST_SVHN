{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import datasets\n",
    "from torchvision import transforms\n",
    "\n",
    "import torchvision\n",
    "import os\n",
    "import pickle\n",
    "import scipy.io\n",
    "import numpy as np\n",
    "\n",
    "from torch.autograd import Variable\n",
    "from torch import optim\n",
    "\n",
    "import argparse\n",
    "import os\n",
    "from torch.backends import cudnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mps\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = \"cuda\"\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = \"mps\"\n",
    "else:\n",
    "    device = \"cpu\"\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def deconv(c_in, c_out, k_size, stride=2, pad=1, bn=True):\n",
    "    \"\"\"Custom deconvolutional layer for simplicity.\"\"\"\n",
    "    layers = []\n",
    "    layers.append(nn.ConvTranspose2d(c_in, c_out, k_size, stride, pad, bias=False))\n",
    "    if bn:\n",
    "        layers.append(nn.BatchNorm2d(c_out))\n",
    "    return nn.Sequential(*layers)\n",
    "\n",
    "def conv(c_in, c_out, k_size, stride=2, pad=1, bn=True):\n",
    "    \"\"\"Custom convolutional layer for simplicity.\"\"\"\n",
    "    layers = []\n",
    "    layers.append(nn.Conv2d(c_in, c_out, k_size, stride, pad, bias=False))\n",
    "    if bn:\n",
    "        layers.append(nn.BatchNorm2d(c_out))\n",
    "    return nn.Sequential(*layers)\n",
    "\n",
    "class G12(nn.Module):\n",
    "    \"\"\"Generator for transfering from mnist to svhn\"\"\"\n",
    "    def __init__(self, conv_dim=64):\n",
    "        super(G12, self).__init__()\n",
    "        # encoding blocks\n",
    "        self.conv1 = conv(1, conv_dim, 4)\n",
    "        self.conv2 = conv(conv_dim, conv_dim*2, 4)\n",
    "        \n",
    "        # residual blocks\n",
    "        self.conv3 = conv(conv_dim*2, conv_dim*2, 3, 1, 1)\n",
    "        self.conv4 = conv(conv_dim*2, conv_dim*2, 3, 1, 1)\n",
    "        \n",
    "        # decoding blocks\n",
    "        self.deconv1 = deconv(conv_dim*2, conv_dim, 4)\n",
    "        self.deconv2 = deconv(conv_dim, 3, 4, bn=False)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = F.leaky_relu(self.conv1(x), 0.05)      # (?, 64, 16, 16)\n",
    "        out = F.leaky_relu(self.conv2(out), 0.05)    # (?, 128, 8, 8)\n",
    "        \n",
    "        out = F.leaky_relu(self.conv3(out), 0.05)    # ( \" )\n",
    "        out = F.leaky_relu(self.conv4(out), 0.05)    # ( \" )\n",
    "        \n",
    "        out = F.leaky_relu(self.deconv1(out), 0.05)  # (?, 64, 16, 16)\n",
    "        out = F.tanh(self.deconv2(out))              # (?, 3, 32, 32)\n",
    "        return out\n",
    "\n",
    "class G21(nn.Module):\n",
    "    \"\"\"Generator for transfering from svhn to mnist\"\"\"\n",
    "    def __init__(self, conv_dim=64):\n",
    "        super(G21, self).__init__()\n",
    "        # encoding blocks\n",
    "        self.conv1 = conv(3, conv_dim, 4)\n",
    "        self.conv2 = conv(conv_dim, conv_dim*2, 4)\n",
    "        \n",
    "        # residual blocks\n",
    "        self.conv3 = conv(conv_dim*2, conv_dim*2, 3, 1, 1)\n",
    "        self.conv4 = conv(conv_dim*2, conv_dim*2, 3, 1, 1)\n",
    "        \n",
    "        # decoding blocks\n",
    "        self.deconv1 = deconv(conv_dim*2, conv_dim, 4)\n",
    "        self.deconv2 = deconv(conv_dim, 1, 4, bn=False)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = F.leaky_relu(self.conv1(x), 0.05)      # (?, 64, 16, 16)\n",
    "        out = F.leaky_relu(self.conv2(out), 0.05)    # (?, 128, 8, 8)\n",
    "        \n",
    "        out = F.leaky_relu(self.conv3(out), 0.05)    # ( \" )\n",
    "        out = F.leaky_relu(self.conv4(out), 0.05)    # ( \" )\n",
    "        \n",
    "        out = F.leaky_relu(self.deconv1(out), 0.05)  # (?, 64, 16, 16)\n",
    "        out = F.tanh(self.deconv2(out))              # (?, 1, 32, 32)\n",
    "        return out\n",
    "\n",
    "class D1(nn.Module):\n",
    "    \"\"\"Discriminator for mnist.\"\"\"\n",
    "    def __init__(self, conv_dim=64, use_labels=False):\n",
    "        super(D1, self).__init__()\n",
    "        self.conv1 = conv(1, conv_dim, 4, bn=False)\n",
    "        self.conv2 = conv(conv_dim, conv_dim*2, 4)\n",
    "        self.conv3 = conv(conv_dim*2, conv_dim*4, 4)\n",
    "        n_out = 11 if use_labels else 1\n",
    "        self.fc = conv(conv_dim*4, n_out, 4, 1, 0, False)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = F.leaky_relu(self.conv1(x), 0.05)    # (?, 64, 16, 16)\n",
    "        out = F.leaky_relu(self.conv2(out), 0.05)  # (?, 128, 8, 8)\n",
    "        out = F.leaky_relu(self.conv3(out), 0.05)  # (?, 256, 4, 4)\n",
    "        out = self.fc(out).squeeze()\n",
    "        return out\n",
    "\n",
    "class D2(nn.Module):\n",
    "    \"\"\"Discriminator for svhn.\"\"\"\n",
    "    def __init__(self, conv_dim=64, use_labels=False):\n",
    "        super(D2, self).__init__()\n",
    "        self.conv1 = conv(3, conv_dim, 4, bn=False)\n",
    "        self.conv2 = conv(conv_dim, conv_dim*2, 4)\n",
    "        self.conv3 = conv(conv_dim*2, conv_dim*4, 4)\n",
    "        n_out = 11 if use_labels else 1\n",
    "        self.fc = conv(conv_dim*4, n_out, 4, 1, 0, False)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = F.leaky_relu(self.conv1(x), 0.05)    # (?, 64, 16, 16)\n",
    "        out = F.leaky_relu(self.conv2(out), 0.05)  # (?, 128, 8, 8)\n",
    "        out = F.leaky_relu(self.conv3(out), 0.05)  # (?, 256, 4, 4)\n",
    "        out = self.fc(out).squeeze()\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_loader():\n",
    "    \"\"\"Builds and returns Dataloader for MNIST and SVHN dataset.\"\"\"\n",
    "    bs_CNN = 64 # batch size\n",
    "    transform = transforms.Compose([\n",
    "    transforms.Resize((32, 32)),  # Resize to 32x32\n",
    "    transforms.Grayscale(num_output_channels=3),  # Convert to RGB by replicating channels\n",
    "    transforms.ToTensor(),  # Convert to tensor\n",
    "    transforms.Normalize((0.5,), (0.5,))  # Normalize each channel (assuming mean 0.5, std 0.5 for simplicity)\n",
    "    ])\n",
    "    \n",
    "    train_dataset_source = datasets.MNIST(root='./mnist_data/', train=True, transform=transform, download=True)\n",
    "    test_dataset_source = datasets.MNIST(root='./mnist_data/', train=False, transform=transform, download=True)\n",
    "\n",
    "    train_dataset_target = datasets.SVHN(root='./svhn_data/', split='train', transform=transform, download=True) # transform to insure same shape and normalisation\n",
    "    test_dataset_target = datasets.SVHN(root='./svhn_data/', split='test', transform=transform, download=True)\n",
    "\n",
    "    \n",
    "    source_loader_train_CNN = torch.utils.data.DataLoader(dataset=train_dataset_source, batch_size=bs_CNN, shuffle=True, drop_last=True)\n",
    "    source_loader_test_CNN = torch.utils.data.DataLoader(dataset=test_dataset_source, batch_size=bs_CNN, shuffle=False, drop_last=True)\n",
    "    target_loader_train_CNN = torch.utils.data.DataLoader(dataset=train_dataset_target, batch_size=bs_CNN, shuffle=True, drop_last=True)\n",
    "    target_loader_test_CNN = torch.utils.data.DataLoader(dataset=test_dataset_target, batch_size=bs_CNN, shuffle=False, drop_last=True)\n",
    "\n",
    "    return source_loader_train_CNN, source_loader_test_CNN, target_loader_train_CNN, target_loader_test_CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Solver(object):\n",
    "    def __init__(self, config, svhn_loader, mnist_loader):\n",
    "        self.svhn_loader = svhn_loader\n",
    "        self.mnist_loader = mnist_loader\n",
    "        self.g12 = None\n",
    "        self.g21 = None\n",
    "        self.d1 = None\n",
    "        self.d2 = None\n",
    "        self.g_optimizer = None\n",
    "        self.d_optimizer = None\n",
    "        self.use_reconst_loss = config.use_reconst_loss\n",
    "        self.use_labels = config.use_labels\n",
    "        self.num_classes = config.num_classes\n",
    "        self.beta1 = config.beta1\n",
    "        self.beta2 = config.beta2\n",
    "        self.g_conv_dim = config.g_conv_dim\n",
    "        self.d_conv_dim = config.d_conv_dim\n",
    "        self.train_iters = config.train_iters\n",
    "        self.batch_size = config.batch_size\n",
    "        self.lr = config.lr\n",
    "        self.log_step = config.log_step\n",
    "        self.sample_step = config.sample_step\n",
    "        self.sample_path = config.sample_path\n",
    "        self.model_path = config.model_path\n",
    "        self.build_model()\n",
    "        \n",
    "    def build_model(self):\n",
    "        \"\"\"Builds a generator and a discriminator.\"\"\"\n",
    "        self.g12 = G12(conv_dim=self.g_conv_dim)\n",
    "        self.g21 = G21(conv_dim=self.g_conv_dim)\n",
    "        self.d1 = D1(conv_dim=self.d_conv_dim, use_labels=self.use_labels)\n",
    "        self.d2 = D2(conv_dim=self.d_conv_dim, use_labels=self.use_labels)\n",
    "        \n",
    "        g_params = list(self.g12.parameters()) + list(self.g21.parameters())\n",
    "        d_params = list(self.d1.parameters()) + list(self.d2.parameters())\n",
    "        \n",
    "        self.g_optimizer = optim.Adam(g_params, self.lr, [self.beta1, self.beta2])\n",
    "        self.d_optimizer = optim.Adam(d_params, self.lr, [self.beta1, self.beta2])\n",
    "        \n",
    "        if torch.cuda.is_available():\n",
    "            self.g12.cuda()\n",
    "            self.g21.cuda()\n",
    "            self.d1.cuda()\n",
    "            self.d2.cuda()\n",
    "    \n",
    "    def merge_images(self, sources, targets, k=10):\n",
    "        _, _, h, w = sources.shape\n",
    "        row = int(np.sqrt(self.batch_size))\n",
    "        merged = np.zeros([3, row*h, row*w*2])\n",
    "        for idx, (s, t) in enumerate(zip(sources, targets)):\n",
    "            i = idx // row\n",
    "            j = idx % row\n",
    "            merged[:, i*h:(i+1)*h, (j*2)*h:(j*2+1)*h] = s\n",
    "            merged[:, i*h:(i+1)*h, (j*2+1)*h:(j*2+2)*h] = t\n",
    "        return merged.transpose(1, 2, 0)\n",
    "    \n",
    "    def to_var(self, x):\n",
    "        \"\"\"Converts numpy to variable.\"\"\"\n",
    "        if torch.cuda.is_available():\n",
    "            x = x.cuda()\n",
    "        return Variable(x)\n",
    "    \n",
    "    def to_data(self, x):\n",
    "        \"\"\"Converts variable to numpy.\"\"\"\n",
    "        if torch.cuda.is_available():\n",
    "            x = x.cpu()\n",
    "        return x.data.numpy()\n",
    "    \n",
    "    def reset_grad(self):\n",
    "        \"\"\"Zeros the gradient buffers.\"\"\"\n",
    "        self.g_optimizer.zero_grad()\n",
    "        self.d_optimizer.zero_grad()\n",
    "\n",
    "    def train(self):\n",
    "        svhn_iter = iter(self.svhn_loader)\n",
    "        mnist_iter = iter(self.mnist_loader)\n",
    "        iter_per_epoch = min(len(svhn_iter), len(mnist_iter))\n",
    "        \n",
    "        # fixed mnist and svhn for sampling\n",
    "        fixed_svhn = self.to_var(svhn_iter.next()[0])\n",
    "        fixed_mnist = self.to_var(mnist_iter.next()[0])\n",
    "        \n",
    "        # loss if use_labels = True\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        \n",
    "        for step in range(self.train_iters+1):\n",
    "            # reset data_iter for each epoch\n",
    "            if (step+1) % iter_per_epoch == 0:\n",
    "                mnist_iter = iter(self.mnist_loader)\n",
    "                svhn_iter = iter(self.svhn_loader)\n",
    "            \n",
    "            # load svhn and mnist dataset\n",
    "            svhn, s_labels = svhn_iter.next() \n",
    "            svhn, s_labels = self.to_var(svhn), self.to_var(s_labels).long().squeeze()\n",
    "            mnist, m_labels = mnist_iter.next() \n",
    "            mnist, m_labels = self.to_var(mnist), self.to_var(m_labels)\n",
    "\n",
    "            if self.use_labels:\n",
    "                mnist_fake_labels = self.to_var(\n",
    "                    torch.Tensor([self.num_classes]*svhn.size(0)).long())\n",
    "                svhn_fake_labels = self.to_var(\n",
    "                    torch.Tensor([self.num_classes]*mnist.size(0)).long())\n",
    "            \n",
    "            #============ train D ============#\n",
    "            \n",
    "            # train with real images\n",
    "            self.reset_grad()\n",
    "            out = self.d1(mnist)\n",
    "            if self.use_labels:\n",
    "                d1_loss = criterion(out, m_labels)\n",
    "            else:\n",
    "                d1_loss = torch.mean((out-1)**2)\n",
    "            \n",
    "            out = self.d2(svhn)\n",
    "            if self.use_labels:\n",
    "                d2_loss = criterion(out, s_labels)\n",
    "            else:\n",
    "                d2_loss = torch.mean((out-1)**2)\n",
    "            \n",
    "            d_mnist_loss = d1_loss\n",
    "            d_svhn_loss = d2_loss\n",
    "            d_real_loss = d1_loss + d2_loss\n",
    "            d_real_loss.backward()\n",
    "            self.d_optimizer.step()\n",
    "            \n",
    "            # train with fake images\n",
    "            self.reset_grad()\n",
    "            fake_svhn = self.g12(mnist)\n",
    "            out = self.d2(fake_svhn)\n",
    "            if self.use_labels:\n",
    "                d2_loss = criterion(out, svhn_fake_labels)\n",
    "            else:\n",
    "                d2_loss = torch.mean(out**2)\n",
    "            \n",
    "            fake_mnist = self.g21(svhn)\n",
    "            out = self.d1(fake_mnist)\n",
    "            if self.use_labels:\n",
    "                d1_loss = criterion(out, mnist_fake_labels)\n",
    "            else:\n",
    "                d1_loss = torch.mean(out**2)\n",
    "            \n",
    "            d_fake_loss = d1_loss + d2_loss\n",
    "            d_fake_loss.backward()\n",
    "            self.d_optimizer.step()\n",
    "            \n",
    "            #============ train G ============#\n",
    "            \n",
    "            # train mnist-svhn-mnist cycle\n",
    "            self.reset_grad()\n",
    "            fake_svhn = self.g12(mnist)\n",
    "            out = self.d2(fake_svhn)\n",
    "            reconst_mnist = self.g21(fake_svhn)\n",
    "            if self.use_labels:\n",
    "                g_loss = criterion(out, m_labels) \n",
    "            else:\n",
    "                g_loss = torch.mean((out-1)**2) \n",
    "\n",
    "            if self.use_reconst_loss:\n",
    "                g_loss += torch.mean((mnist - reconst_mnist)**2)\n",
    "\n",
    "            g_loss.backward()\n",
    "            self.g_optimizer.step()\n",
    "\n",
    "            # train svhn-mnist-svhn cycle\n",
    "            self.reset_grad()\n",
    "            fake_mnist = self.g21(svhn)\n",
    "            out = self.d1(fake_mnist)\n",
    "            reconst_svhn = self.g12(fake_mnist)\n",
    "            if self.use_labels:\n",
    "                g_loss = criterion(out, s_labels) \n",
    "            else:\n",
    "                g_loss = torch.mean((out-1)**2) \n",
    "\n",
    "            if self.use_reconst_loss:\n",
    "                g_loss += torch.mean((svhn - reconst_svhn)**2)\n",
    "\n",
    "            g_loss.backward()\n",
    "            self.g_optimizer.step()\n",
    "            \n",
    "            # print the log info\n",
    "            if (step+1) % self.log_step == 0:\n",
    "                print('Step [%d/%d], d_real_loss: %.4f, d_mnist_loss: %.4f, d_svhn_loss: %.4f, '\n",
    "                      'd_fake_loss: %.4f, g_loss: %.4f' \n",
    "                      %(step+1, self.train_iters, d_real_loss.data[0], d_mnist_loss.data[0], \n",
    "                        d_svhn_loss.data[0], d_fake_loss.data[0], g_loss.data[0]))\n",
    "\n",
    "            # save the sampled images\n",
    "            if (step+1) % self.sample_step == 0:\n",
    "                fake_svhn = self.g12(fixed_mnist)\n",
    "                fake_mnist = self.g21(fixed_svhn)\n",
    "                \n",
    "                mnist, fake_mnist = self.to_data(fixed_mnist), self.to_data(fake_mnist)\n",
    "                svhn , fake_svhn = self.to_data(fixed_svhn), self.to_data(fake_svhn)\n",
    "                \n",
    "                merged = self.merge_images(mnist, fake_svhn)\n",
    "                path = os.path.join(self.sample_path, 'sample-%d-m-s.png' %(step+1))\n",
    "                scipy.misc.imsave(path, merged)\n",
    "                print ('saved %s' %path)\n",
    "                \n",
    "                merged = self.merge_images(svhn, fake_mnist)\n",
    "                path = os.path.join(self.sample_path, 'sample-%d-s-m.png' %(step+1))\n",
    "                scipy.misc.imsave(path, merged)\n",
    "                print ('saved %s' %path)\n",
    "            \n",
    "            if (step+1) % 5000 == 0:\n",
    "                # save the model parameters for each epoch\n",
    "                g12_path = os.path.join(self.model_path, 'g12-%d.pkl' %(step+1))\n",
    "                g21_path = os.path.join(self.model_path, 'g21-%d.pkl' %(step+1))\n",
    "                d1_path = os.path.join(self.model_path, 'd1-%d.pkl' %(step+1))\n",
    "                d2_path = os.path.join(self.model_path, 'd2-%d.pkl' %(step+1))\n",
    "                torch.save(self.g12.state_dict(), g12_path)\n",
    "                torch.save(self.g21.state_dict(), g21_path)\n",
    "                torch.save(self.d1.state_dict(), d1_path)\n",
    "                torch.save(self.d2.state_dict(), d2_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {      \"image_size\":           32,\n",
    "                \"g_conv_dim\":           64,\n",
    "                \"d_conv_dim\":           64,\n",
    "                \"use_reconst_loss\":     True,\n",
    "                \"use_labels\":           False,\n",
    "                \"num_classes\":          10,\n",
    "                \"train_iters\":          40000,\n",
    "                \"batch_size\":           64,\n",
    "                \"num_workers\":          2,\n",
    "                \"lr\":                   0.0002,\n",
    "                \"beta1\":                0.5,\n",
    "                \"beta2\":                0.999,\n",
    "                \"mode\":                 \"train\", \n",
    "                \"model_path\":           \"./models\",\n",
    "                \"sample_path\":          \"./samples\",\n",
    "                \"mnist_path\":           \"./mnist\",\n",
    "                \"svhn_path\":            \"./svhn\",\n",
    "                \"log_step\":             10,\n",
    "                \"sample_step\":          500\n",
    "                                                    }\n",
    "\n",
    "class DotDict:\n",
    "    def __init__(self, data):\n",
    "        self._data = data\n",
    "\n",
    "    def __getattr__(self, name):\n",
    "        if name in self._data:\n",
    "            return self._data[name]\n",
    "        else:\n",
    "            raise AttributeError(f\"'{type(self).__name__}' object has no attribute '{name}'\")\n",
    "\n",
    "config = DotDict(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using downloaded and verified file: ./svhn_data/train_32x32.mat\n",
      "Using downloaded and verified file: ./svhn_data/test_32x32.mat\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'dict' object has no attribute 'use_reconst_loss'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# svhn_loader, mnist_loader = get_loader()\u001b[39;00m\n\u001b[1;32m      2\u001b[0m source_loader_train_CNN, source_loader_test_CNN, target_loader_train_CNN, target_loader_test_CNN \u001b[38;5;241m=\u001b[39m get_loader()\n\u001b[0;32m----> 4\u001b[0m solver \u001b[38;5;241m=\u001b[39m \u001b[43mSolver\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget_loader_train_CNN\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msource_loader_train_CNN\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      5\u001b[0m cudnn\u001b[38;5;241m.\u001b[39mbenchmark \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m \n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# create directories if not exist\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[9], line 11\u001b[0m, in \u001b[0;36mSolver.__init__\u001b[0;34m(self, config, svhn_loader, mnist_loader)\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mg_optimizer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39md_optimizer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m---> 11\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39muse_reconst_loss \u001b[38;5;241m=\u001b[39m \u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43muse_reconst_loss\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39muse_labels \u001b[38;5;241m=\u001b[39m config\u001b[38;5;241m.\u001b[39muse_labels\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_classes \u001b[38;5;241m=\u001b[39m config\u001b[38;5;241m.\u001b[39mnum_classes\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'dict' object has no attribute 'use_reconst_loss'"
     ]
    }
   ],
   "source": [
    "# svhn_loader, mnist_loader = get_loader()\n",
    "source_loader_train_CNN, source_loader_test_CNN, target_loader_train_CNN, target_loader_test_CNN = get_loader()\n",
    "\n",
    "solver = Solver(config, target_loader_train_CNN, source_loader_train_CNN)\n",
    "cudnn.benchmark = True \n",
    "\n",
    "# create directories if not exist\n",
    "if not os.path.exists(config.model_path):\n",
    "    os.makedirs(config.model_path)\n",
    "if not os.path.exists(config.sample_path):\n",
    "    os.makedirs(config.sample_path)\n",
    "\n",
    "if config.mode == 'train':\n",
    "    solver.train()\n",
    "elif config.mode == 'sample':\n",
    "    solver.sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "8abd2a7b1dc99e88aaddfe56a47290d4ccb19c13e631b171a8c192109287e869"
  },
  "kernelspec": {
   "display_name": "Python 3.11.7 ('.venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
